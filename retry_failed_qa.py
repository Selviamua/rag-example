"""é‡è¯•è„šæœ¬ - å¤„ç†å¤±è´¥çš„ 5 æ¡é—®é¢˜ (ID: 7, 8, 10, 11, 22)"""

import os
import json
from datetime import datetime
from dotenv import load_dotenv

# Load the environment variables from the .env file
load_dotenv()

from langchain_openai import ChatOpenAI
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.chat_models import AzureChatOpenAI, BedrockChat
from langchain_community.vectorstores import Chroma, OpenSearchVectorSearch
from langchain_community.vectorstores.pgvector import PGVector
from langchain_classic.chains import ConversationalRetrievalChain
from langchain_classic.memory import ConversationBufferWindowMemory

# Log full text sent to LLM
VERBOSE = False

# Details of persisted embedding store index
COLLECTION_NAME = "doc_index"
EMBEDDING_MODEL = "all-MiniLM-L6-v2"
MEMORY_WINDOW_SIZE = 10

# éœ€è¦é‡è¯•çš„ ID
RETRY_IDS = [7, 8, 10, 11, 22]

# åŸå§‹ç»“æœæ–‡ä»¶å’Œè¾“å‡ºæ–‡ä»¶
ORIGINAL_RESULTS_FILE = "qa_results_20260106_223043.json"
OUTPUT_FILE = f"qa_results_complete_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"


def evaluate_answer_correctness(answer, gold_answer):
    """è¯„ä¼°ç­”æ¡ˆæ˜¯å¦æ­£ç¡®"""
    answer = answer.strip()
    gold_answer = gold_answer.strip()
    
    if answer == gold_answer:
        return True
    
    if gold_answer in answer:
        return True
    
    import re
    gold_numbers = re.findall(r'\d+\.?\d*', gold_answer)
    answer_numbers = re.findall(r'\d+\.?\d*', answer)
    if gold_numbers and gold_numbers == answer_numbers[:len(gold_numbers)]:
        return True
    
    return False


def load_llm():
    """åŠ è½½è¯­è¨€æ¨¡å‹"""
    deepseek_api_key = os.getenv("DEEPSEEK_API_KEY")
    deepseek_model = os.getenv("DEEPSEEK_MODEL", "deepseek-chat")
    deepseek_base_url = os.getenv("DEEPSEEK_BASE_URL", "https://api.deepseek.com")
    
    openai_model_name = os.getenv("OPENAI_MODEL_NAME")
    aws_credential_profile_name = os.getenv("AWS_CREDENTIAL_PROFILE_NAME")
    aws_bedrock_model_name = os.getenv("AWS_BEDROCK_MODEL_NAME")
    
    if deepseek_api_key:
        print("Using DeepSeek for language model.")
        return ChatOpenAI(
            model=deepseek_model,
            api_key=deepseek_api_key,
            base_url=deepseek_base_url,
            temperature=0.5,
            verbose=VERBOSE,
        )
    elif openai_model_name:
        print("Using Azure for language model.")
        return AzureChatOpenAI(
            temperature=0.5, deployment_name=openai_model_name, verbose=VERBOSE
        )
    elif aws_credential_profile_name and aws_bedrock_model_name:
        print("Using Amazon Bedrock for language model.")
        return BedrockChat(
            credentials_profile_name=aws_credential_profile_name,
            model_id=aws_bedrock_model_name,
            verbose=VERBOSE,
        )
    else:
        raise EnvironmentError("No language model environment variables found.")


def get_embed_db(embeddings):
    """è·å–å‘é‡æ•°æ®åº“"""
    chroma_persist_dir = os.getenv("CHROMA_PERSIST_DIR")
    opensearch_url = os.getenv("OPENSEARCH_URL")
    postgres_conn = os.getenv("POSTGRES_CONNECTION")
    
    if chroma_persist_dir:
        db = Chroma(
            embedding_function=embeddings,
            collection_name=COLLECTION_NAME,
            persist_directory=chroma_persist_dir,
        )
    elif opensearch_url:
        username = os.getenv("OPENSEARCH_USERNAME")
        password = os.getenv("OPENSEARCH_PASSWORD")
        db = OpenSearchVectorSearch(
            embedding_function=embeddings,
            index_name=COLLECTION_NAME,
            opensearch_url=opensearch_url,
            http_auth=(username, password),
            use_ssl=False,
            verify_certs=False,
            ssl_assert_hostname=False,
            ssl_show_warn=False,
        )
    elif postgres_conn:
        db = PGVector(
            embedding_function=embeddings,
            collection_name=COLLECTION_NAME,
            connection_string=postgres_conn,
        )
    else:
        raise EnvironmentError("No vector store environment variables found.")
    return db


def load_original_results(filename):
    """åŠ è½½åŸå§‹ç»“æœ"""
    try:
        with open(filename, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {filename}")
        return []
    except json.JSONDecodeError as e:
        print(f"âŒ é”™è¯¯: JSON è§£æå¤±è´¥ - {e}")
        return []


def retry_failed_questions(benchmark_file, query_chain, db, original_results):
    """é‡æ–°è¿è¡Œå¤±è´¥çš„é—®é¢˜"""
    # ä»åŸå§‹ç»“æœä¸­æå–å¤±è´¥çš„é—®é¢˜ä¿¡æ¯
    benchmark_data = load_original_results(benchmark_file)
    
    retry_results = {}  # {id: result_dict}
    
    for item in benchmark_data:
        item_id = item.get("id")
        if item_id not in RETRY_IDS:
            continue
        
        question_text = item.get("question")
        gold_answer = item.get("gold_answer", "")
        
        print(f"\n{'='*80}")
        print(f"ğŸ”„ é‡è¯• ID: {item_id}")
        print(f"é—®é¢˜: {question_text}")
        
        try:
            # ä½¿ç”¨ RAG ç³»ç»Ÿç”Ÿæˆç­”æ¡ˆ
            response = query_chain.invoke({"question": question_text})
            answer = response["answer"]
            
            # è·å–ç›¸ä¼¼åº¦åˆ†æ•°
            docs_with_scores = db.similarity_search_with_score(question_text, k=3)
            
            sources = []
            for doc, score in docs_with_scores:
                sources.append({
                    "page": doc.metadata.get("page", "N/A"),
                    "source": doc.metadata.get("source", "N/A"),
                    "similarity_score": float(score)
                })
            
            # è¯„ä¼°ç­”æ¡ˆ
            answer_correctness = evaluate_answer_correctness(answer, gold_answer)
            
            result = {
                "id": item_id,
                "question": question_text,
                "answer": answer,
                "gold_answer": gold_answer,
                "answer_correctness": answer_correctness,
                "sources": sources
            }
            
            retry_results[item_id] = result
            
            # æ‰“å°ç»“æœ
            answer_preview = answer[:100] + "..." if len(answer) > 100 else answer
            status = "âœ“" if answer_correctness else "âœ—"
            print(f"âœ… æˆåŠŸé‡è¯•")
            print(f"ç­”æ¡ˆé¢„è§ˆ: {answer_preview}")
            print(f"æ­£ç¡®æ€§: {status}")
            print(f"ç›¸ä¼¼åº¦åˆ†æ•°: {[round(s['similarity_score'], 3) for s in sources]}")
            
        except Exception as e:
            print(f"âŒ é”™è¯¯: é‡è¯•å¤±è´¥ - {e}")
            retry_results[item_id] = {
                "id": item_id,
                "question": question_text,
                "answer": f"ERROR: {str(e)}",
                "gold_answer": gold_answer,
                "answer_correctness": False,
                "sources": []
            }
    
    return retry_results


def merge_results(original_results, retry_results):
    """åˆå¹¶åŸå§‹ç»“æœå’Œé‡è¯•ç»“æœ"""
    merged = []
    
    for item in original_results:
        item_id = item.get("id")
        if item_id in retry_results:
            # ç”¨é‡è¯•ç»“æœæ›¿æ¢
            merged.append(retry_results[item_id])
        else:
            # ä¿æŒåŸå§‹ç»“æœ
            merged.append(item)
    
    return merged


def main():
    print("="*80)
    print("ğŸ”„ é‡è¯•å¤±è´¥é—®é¢˜è„šæœ¬å¯åŠ¨")
    print("="*80)
    
    # 1. åŠ è½½åŸå§‹ç»“æœ
    print(f"\n1ï¸âƒ£  åŠ è½½åŸå§‹ç»“æœ: {ORIGINAL_RESULTS_FILE}")
    original_results = load_original_results(ORIGINAL_RESULTS_FILE)
    if not original_results:
        print("âŒ æ— æ³•åŠ è½½åŸå§‹ç»“æœï¼Œç¨‹åºé€€å‡ºã€‚")
        return
    print(f"âœ… æˆåŠŸåŠ è½½ {len(original_results)} æ¡ç»“æœ")
    
    # 2. åˆå§‹åŒ– RAG ç³»ç»Ÿ
    print("\n2ï¸âƒ£  åˆå§‹åŒ– RAG ç³»ç»Ÿ...")
    embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)
    db = get_embed_db(embeddings)
    retriever = db.as_retriever()
    llm = load_llm()
    
    # 3. åˆ›å»ºå¯¹è¯é“¾
    memory = ConversationBufferWindowMemory(
        memory_key="chat_history",
        output_key="answer",
        return_messages=True,
        window_size=MEMORY_WINDOW_SIZE,
    )
    
    query_chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        memory=memory,
        retriever=retriever,
        verbose=VERBOSE,
        return_source_documents=True,
    )
    
    # 4. é‡è¯•å¤±è´¥çš„é—®é¢˜
    print(f"\n3ï¸âƒ£  é‡è¯•å¤±è´¥çš„é—®é¢˜ (ID: {RETRY_IDS})...")
    benchmark_file = "benchmark.json"
    retry_results = retry_failed_questions(benchmark_file, query_chain, db, original_results)
    
    # 5. åˆå¹¶ç»“æœ
    print("\n4ï¸âƒ£  åˆå¹¶åŸå§‹ç»“æœå’Œé‡è¯•ç»“æœ...")
    merged_results = merge_results(original_results, retry_results)
    
    # 6. ä¿å­˜æœ€ç»ˆç»“æœ
    print(f"\n5ï¸âƒ£  ä¿å­˜æœ€ç»ˆç»“æœåˆ°: {OUTPUT_FILE}")
    try:
        with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
            json.dump(merged_results, f, ensure_ascii=False, indent=2)
        print(f"âœ… æˆåŠŸä¿å­˜å®Œæ•´ç»“æœ!")
        print(f"   æ€»æ¡æ•°: {len(merged_results)}")
        print(f"   æˆåŠŸé‡è¯•: {len(retry_results)} æ¡")
        print(f"   ä¿å­˜ä½ç½®: {OUTPUT_FILE}")
    except Exception as e:
        print(f"âŒ ä¿å­˜å¤±è´¥: {e}")
    
    print("\n" + "="*80)
    print("âœ… é‡è¯•è„šæœ¬å®Œæˆï¼")
    print("="*80)


if __name__ == "__main__":
    main()
