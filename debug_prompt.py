"""Prompt è¯Šæ–­è„šæœ¬ - åˆ†æç”Ÿæˆé˜¶æ®µçš„é—®é¢˜
ä¸“æ³¨äº ID 5, 7ï¼šæ£€ç´¢æˆåŠŸä½†ç”Ÿæˆå¤±è´¥çš„æ¡ˆä¾‹
"""

import os
import json
import numpy as np
from dotenv import load_dotenv

load_dotenv()

from langchain_openai import ChatOpenAI
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_classic.chains import ConversationalRetrievalChain
from langchain_classic.memory import ConversationBufferWindowMemory

COLLECTION_NAME = "doc_index"
EMBEDDING_MODEL = "all-MiniLM-L6-v2"
VERBOSE = True  # å¼€å¯è¯¦ç»†æ—¥å¿—


def get_embed_db(embeddings):
    """è·å–å‘é‡æ•°æ®åº“"""
    chroma_persist_dir = os.getenv("CHROMA_PERSIST_DIR")
    db = Chroma(
        embedding_function=embeddings,
        collection_name=COLLECTION_NAME,
        persist_directory=chroma_persist_dir,
    )
    return db


def load_llm(temperature=0.5):
    """åŠ è½½ LLM"""
    deepseek_api_key = os.getenv("DEEPSEEK_API_KEY")
    deepseek_model = os.getenv("DEEPSEEK_MODEL", "deepseek-chat")
    deepseek_base_url = os.getenv("DEEPSEEK_BASE_URL", "https://api.deepseek.com")
    
    return ChatOpenAI(
        model=deepseek_model,
        api_key=deepseek_api_key,
        base_url=deepseek_base_url,
        temperature=temperature,
        verbose=VERBOSE,
    )


def load_benchmark(benchmark_file):
    """åŠ è½½ benchmark æ•°æ®"""
    with open(benchmark_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    return {item['id']: item for item in data}


def estimate_tokens(text):
    """ç²—ç•¥ä¼°ç®— token æ•°é‡ï¼ˆè‹±æ–‡çº¦ä¸ºå­—æ•°/4ï¼‰"""
    return len(text.split()) // 4 + len(text) // 4


def analyze_prompt_generation(question_id, benchmark_data, temperature=0.5):
    """åˆ†æå•ä¸ªé—®é¢˜çš„ Prompt å’Œç”Ÿæˆè¿‡ç¨‹"""
    
    if question_id not in benchmark_data:
        print(f"âŒ é—®é¢˜ ID {question_id} ä¸å­˜åœ¨\n")
        return
    
    item = benchmark_data[question_id]
    question = item['question']
    gold_answer = item['gold_answer']
    
    print("\n" + "="*80)
    print(f"ã€é—®é¢˜ ID: {question_id}ã€‘")
    print("="*80)
    
    # ========== 1. åŸºæœ¬ä¿¡æ¯ ==========
    print(f"\nã€1ï¸âƒ£  é—®é¢˜ã€‘")
    print(f"   {question}")
    
    print(f"\nã€ğŸ“‹ é‡‘æ ‡ç­”æ¡ˆã€‘")
    print(f"   {gold_answer}")
    
    # ========== 2. æ£€ç´¢é˜¶æ®µ ==========
    print(f"\nã€2ï¸âƒ£  æ£€ç´¢é˜¶æ®µ - Top-3 æ–‡æ®µã€‘")
    print("-" * 80)
    
    embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)
    db = get_embed_db(embeddings)
    
    docs_with_scores = db.similarity_search_with_score(question, k=3)
    
    retrieved_texts = []
    total_retrieved_length = 0
    
    for rank, (doc, _) in enumerate(docs_with_scores, 1):
        content = doc.page_content
        retrieved_texts.append(content)
        total_retrieved_length += len(content)
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        query_emb = np.array(embeddings.embed_query(question))
        doc_emb = np.array(embeddings.embed_query(content))
        cosine_sim = np.dot(query_emb, doc_emb) / (
            np.linalg.norm(query_emb) * np.linalg.norm(doc_emb)
        )
        
        print(f"\n   ã€ç¬¬ {rank} åã€‘ç›¸ä¼¼åº¦: {cosine_sim:.4f}")
        print(f"   é¡µç : {doc.metadata.get('page', 'N/A')}")
        print(f"   æ–‡æ®µé•¿åº¦: {len(content)} å­—ç¬¦")
        print(f"   å†…å®¹é¢„è§ˆ: {content[:200]}...")
    
    print(f"\n   æ€»æ£€ç´¢é•¿åº¦: {total_retrieved_length} å­—ç¬¦")
    print(f"   ä¼°ç®— tokens: ~{estimate_tokens(''.join(retrieved_texts))}")
    
    # ========== 3. ç”Ÿæˆé˜¶æ®µ ==========
    print(f"\nã€3ï¸âƒ£  ç”Ÿæˆé˜¶æ®µ - è°ƒç”¨ LLMã€‘")
    print("-" * 80)
    print(f"   Temperature: {temperature}")
    
    # åˆ›å»º RAG é“¾
    retriever = db.as_retriever()
    llm = load_llm(temperature=temperature)
    
    memory = ConversationBufferWindowMemory(
        memory_key="chat_history",
        output_key="answer",
        return_messages=True,
        window_size=10,
    )
    
    query_chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        memory=memory,
        retriever=retriever,
        verbose=VERBOSE,
        return_source_documents=True,
    )
    
    print(f"\n   ã€è°ƒç”¨ LLM...ã€‘")
    response = query_chain.invoke({"question": question})
    
    generated_answer = response["answer"]
    source_docs = response.get("source_documents", [])
    
    print(f"\n   ã€ç”Ÿæˆçš„ç­”æ¡ˆã€‘")
    print(f"   {generated_answer}")
    
    print(f"\n   ç­”æ¡ˆé•¿åº¦: {len(generated_answer)} å­—ç¬¦")
    print(f"   ä¼°ç®— tokens: ~{estimate_tokens(generated_answer)}")
    
    # ========== 4. å¯¹æ¯”åˆ†æ ==========
    print(f"\nã€4ï¸âƒ£  å¯¹æ¯”åˆ†æã€‘")
    print("-" * 80)
    
    # æ£€æŸ¥é‡‘æ ‡ç­”æ¡ˆçš„å…³é”®ä¿¡æ¯æ˜¯å¦åœ¨æ£€ç´¢æ–‡æ®µä¸­
    gold_keywords = gold_answer.lower().split()[:10]  # å–å‰10ä¸ªè¯ä½œä¸ºå…³é”®è¯
    
    found_in_retrieval = []
    for kw in gold_keywords:
        for text in retrieved_texts:
            if kw in text.lower():
                found_in_retrieval.append(kw)
                break
    
    retrieval_coverage = len(found_in_retrieval) / len(gold_keywords) if gold_keywords else 0
    
    print(f"\n   ã€æ£€ç´¢è¦†ç›–åº¦ã€‘")
    print(f"   é‡‘æ ‡ç­”æ¡ˆçš„å…³é”®è¯åœ¨æ£€ç´¢æ–‡æ®µä¸­çš„è¦†ç›–: {retrieval_coverage*100:.1f}%")
    print(f"   æ‰¾åˆ°çš„å…³é”®è¯: {found_in_retrieval[:5]}...")
    
    # æ£€æŸ¥ç”Ÿæˆç­”æ¡ˆæ˜¯å¦åŒ…å«é‡‘æ ‡ç­”æ¡ˆçš„å…³é”®ä¿¡æ¯
    answer_match = gold_answer.lower() in generated_answer.lower()
    
    print(f"\n   ã€ç”Ÿæˆè´¨é‡ã€‘")
    if answer_match:
        print(f"   âœ… ç”Ÿæˆç­”æ¡ˆåŒ…å«é‡‘æ ‡ç­”æ¡ˆ")
    else:
        print(f"   âŒ ç”Ÿæˆç­”æ¡ˆæœªåŒ…å«é‡‘æ ‡ç­”æ¡ˆ")
    
    # æ£€æŸ¥ç”Ÿæˆç­”æ¡ˆæ˜¯å¦ä½¿ç”¨äº†æ£€ç´¢å†…å®¹
    uses_retrieval = False
    for text in retrieved_texts:
        # å–æ£€ç´¢æ–‡æ®µçš„ç‰¹å¾ç‰‡æ®µ
        unique_phrase = ' '.join(text.split()[:10])
        if unique_phrase.lower() in generated_answer.lower():
            uses_retrieval = True
            break
    
    if uses_retrieval:
        print(f"   âœ… ç”Ÿæˆç­”æ¡ˆä½¿ç”¨äº†æ£€ç´¢å†…å®¹")
    else:
        print(f"   âš ï¸  ç”Ÿæˆç­”æ¡ˆå¯èƒ½æœªå……åˆ†ä½¿ç”¨æ£€ç´¢å†…å®¹")
    
    # ========== 5. è¯Šæ–­ç»“è®º ==========
    print(f"\nã€ğŸ” è¯Šæ–­ç»“è®ºã€‘")
    print("-" * 80)
    
    if retrieval_coverage > 0.7:
        print(f"   âœ… æ£€ç´¢è´¨é‡ä¼˜ç§€ï¼šæ–‡æ®µåŒ…å« {retrieval_coverage*100:.0f}% çš„ç­”æ¡ˆå…³é”®è¯")
    elif retrieval_coverage > 0.4:
        print(f"   âš ï¸  æ£€ç´¢è´¨é‡ä¸­ç­‰ï¼šæ–‡æ®µåŒ…å« {retrieval_coverage*100:.0f}% çš„ç­”æ¡ˆå…³é”®è¯")
    else:
        print(f"   âŒ æ£€ç´¢è´¨é‡å·®ï¼šæ–‡æ®µä»…åŒ…å« {retrieval_coverage*100:.0f}% çš„ç­”æ¡ˆå…³é”®è¯")
    
    if not answer_match:
        if uses_retrieval:
            print(f"\n   ã€è¯Šæ–­ã€‘ï¼šLLM ä½¿ç”¨äº†æ£€ç´¢å†…å®¹ï¼Œä½†ç”Ÿæˆäº†é”™è¯¯ç­”æ¡ˆ")
            print(f"   å¯èƒ½åŸå› ï¼š")
            print(f"     1. æ£€ç´¢æ–‡æ®µä¸­çš„ä¿¡æ¯ä¸å¤Ÿå‡†ç¡®æˆ–å®Œæ•´")
            print(f"     2. LLM å¯¹æ£€ç´¢å†…å®¹çš„ç†è§£æœ‰åå·®")
            print(f"     3. Temperature={temperature} å¯¼è‡´ç”Ÿæˆè¿‡äºåˆ›æ„")
            print(f"\n   ã€å»ºè®®ã€‘ï¼š")
            print(f"     - å°è¯•é™ä½ temperature (â†’ 0.1 æˆ– 0.0)")
            print(f"     - ä¼˜åŒ– Prompt æ¨¡æ¿ï¼Œæ˜ç¡®è¦æ±‚åŸºäºæ£€ç´¢å†…å®¹å›ç­”")
        else:
            print(f"\n   ã€è¯Šæ–­ã€‘ï¼šLLM å¯èƒ½å¿½ç•¥äº†æ£€ç´¢å†…å®¹ï¼Œè‡ªå·±ç¼–é€ ç­”æ¡ˆ")
            print(f"   å¯èƒ½åŸå› ï¼š")
            print(f"     1. Prompt æ¨¡æ¿ä¸æ¸…æ™°ï¼ŒLLM æ²¡ç†è§£è¦ç”¨æ£€ç´¢å†…å®¹")
            print(f"     2. æ£€ç´¢æ–‡æ®µä¸é—®é¢˜å…³è”æ€§ä¸å¼º")
            print(f"     3. Temperature è¿‡é«˜å¯¼è‡´è¿‡åº¦åˆ›æ„")
            print(f"\n   ã€å»ºè®®ã€‘ï¼š")
            print(f"     - æ£€æŸ¥å¹¶ä¼˜åŒ– ConversationalRetrievalChain çš„ system prompt")
            print(f"     - é™ä½ temperature")
            print(f"     - è€ƒè™‘ä½¿ç”¨é‡æ’ï¼ˆrerankerï¼‰æå‡æ£€ç´¢è´¨é‡")
    else:
        print(f"\n   âœ… ç”ŸæˆæˆåŠŸï¼ç­”æ¡ˆæ­£ç¡®ã€‚")
    
    print("\n")


def compare_temperatures(question_id, benchmark_data):
    """å¯¹æ¯”ä¸åŒ temperature çš„ç”Ÿæˆæ•ˆæœ"""
    
    print("\n" + "="*80)
    print(f"ã€Temperature å¯¹æ¯”å®éªŒ - ID {question_id}ã€‘")
    print("="*80)
    
    temperatures = [0.5, 0.1, 0.0]
    
    for temp in temperatures:
        print(f"\n{'â”€'*80}")
        print(f"ã€Temperature = {temp}ã€‘")
        print(f"{'â”€'*80}")
        
        analyze_prompt_generation(question_id, benchmark_data, temperature=temp)
        
        print("\nâ³ ç­‰å¾… 3 ç§’åç»§ç»­...\n")
        import time
        time.sleep(3)


def main():
    print("\n" + "="*80)
    print("ã€RAG Prompt & ç”Ÿæˆé˜¶æ®µè¯Šæ–­å·¥å…·ã€‘")
    print("ä¸“æ³¨äºæ£€ç´¢æˆåŠŸä½†ç”Ÿæˆå¤±è´¥çš„æ¡ˆä¾‹ (ID 5, 7)")
    print("="*80)
    
    benchmark_data = load_benchmark("benchmark.json")
    
    # é‡ç‚¹åˆ†æçš„ä¸¤ä¸ªé—®é¢˜
    problem_ids = [5, 7]
    
    print(f"\nã€å•æ¬¡è¯Šæ–­ã€‘")
    for qid in problem_ids:
        analyze_prompt_generation(qid, benchmark_data, temperature=0.5)
    
    # å¯é€‰ï¼šå¯¹æ¯”ä¸åŒ temperature
    print(f"\n" + "="*80)
    print("æ˜¯å¦è¿›è¡Œ Temperature å¯¹æ¯”å®éªŒï¼Ÿ(éœ€è¦æ›´å¤šæ—¶é—´å’Œ API è°ƒç”¨)")
    print("="*80)
    # é»˜è®¤ä¸è¿è¡Œï¼Œå¦‚éœ€è¿è¡Œå¯å–æ¶ˆæ³¨é‡Š
    # compare_temperatures(5, benchmark_data)


if __name__ == "__main__":
    main()
