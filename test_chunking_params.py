"""æµ‹è¯•ä¸åŒçš„ chunking å‚æ•°å¯¹æ£€ç´¢è´¨é‡çš„å½±å“"""

import os
import json
from datetime import datetime
from dotenv import load_dotenv
from transformers import AutoTokenizer
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
import numpy as np

load_dotenv()

# æµ‹è¯•é…ç½®
TEST_CONFIGS = [
    # åŸå§‹é…ç½®
    {"chunk_size": 512, "chunk_overlap": 0, "name": "åŸå§‹ (512/0)"},
    # å° chunk + ä¸­ç­‰ overlap
    {"chunk_size": 256, "chunk_overlap": 50, "name": "å°å— (256/50)"},
    {"chunk_size": 256, "chunk_overlap": 100, "name": "å°å— (256/100)"},
    # ä¸­ç­‰ chunk + ä¸åŒ overlap
    {"chunk_size": 512, "chunk_overlap": 50, "name": "ä¸­å— (512/50)"},
    {"chunk_size": 512, "chunk_overlap": 100, "name": "ä¸­å— (512/100)"},
    {"chunk_size": 512, "chunk_overlap": 150, "name": "ä¸­å— (512/150)"},
    # å¤§ chunk + ä¸åŒ overlap
    {"chunk_size": 1024, "chunk_overlap": 100, "name": "å¤§å— (1024/100)"},
    {"chunk_size": 1024, "chunk_overlap": 200, "name": "å¤§å— (1024/200)"},
]

# æµ‹è¯•é—®é¢˜ (ID 5 å’Œ 7)
TEST_QUESTIONS = [
    {
        "id": 5,
        "question": "What are the limitations of bioplastic based on current microorganisms?",
        "gold_answer": "Lower tolerances to harsh industrial conditions compared to their synthetic counterparts."
    },
    {
        "id": 7,
        "question": "How can extremophiles help overcome these limitations?",
        "gold_answer": "Extremophiles have adaptations that allow them to thrive in extreme conditions, which could be harnessed to produce bioplastics with improved properties."
    }
]

EMBEDDING_MODEL = "all-MiniLM-L6-v2"
SOURCE_PDF = "source_documents/Le.pdf"


def create_index_with_params(chunk_size, chunk_overlap, collection_name):
    """ä½¿ç”¨æŒ‡å®šå‚æ•°åˆ›å»ºå‘é‡ç´¢å¼•"""
    print(f"  ğŸ“„ åŠ è½½ PDF...")
    loader = PyPDFLoader(SOURCE_PDF)
    
    print(f"  ğŸ”ª åˆ‡åˆ†æ–‡æ¡£ (size={chunk_size}, overlap={chunk_overlap})...")
    tokenizer = AutoTokenizer.from_pretrained("sentence-transformers/all-MiniLM-L6-v2")
    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(
        tokenizer,
        separators=["\n \n", "\n\n", "\n", " ", ""],
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
    )
    docs = loader.load_and_split(text_splitter)
    print(f"  âœ“ å…±åˆ‡åˆ†ä¸º {len(docs)} ä¸ªæ–‡æ¡£å—")
    
    print(f"  ğŸ”¢ ç”ŸæˆåµŒå…¥å‘é‡...")
    embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)
    
    chroma_persist_dir = os.getenv("CHROMA_PERSIST_DIR")
    db = Chroma.from_documents(
        documents=docs,
        embedding=embeddings,
        collection_name=collection_name,
        persist_directory=chroma_persist_dir,
    )
    db.persist()
    
    return db, embeddings, len(docs)


def calculate_cosine_similarity(query_embedding, doc_embedding):
    """æ‰‹åŠ¨è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
    query_norm = np.linalg.norm(query_embedding)
    doc_norm = np.linalg.norm(doc_embedding)
    if query_norm == 0 or doc_norm == 0:
        return 0.0
    return np.dot(query_embedding, doc_embedding) / (query_norm * doc_norm)


def test_retrieval(db, embeddings, question, question_id):
    """æµ‹è¯•æ£€ç´¢è´¨é‡"""
    query_embedding = embeddings.embed_query(question["question"])
    
    # è·å– Top-3 æ–‡æ¡£åŠå…¶åµŒå…¥
    results = db.similarity_search_with_score(question["question"], k=3)
    
    retrieval_info = {
        "question_id": question_id,
        "top_docs": []
    }
    
    similarities = []
    for i, (doc, _) in enumerate(results):
        doc_embedding = embeddings.embed_query(doc.page_content)
        similarity = calculate_cosine_similarity(query_embedding, doc_embedding)
        similarities.append(similarity)
        
        retrieval_info["top_docs"].append({
            "rank": i + 1,
            "similarity": float(similarity),
            "page": doc.metadata.get("page", "N/A"),
            "content_preview": doc.page_content[:100] + "..."
        })
    
    retrieval_info["max_similarity"] = float(max(similarities))
    retrieval_info["avg_similarity"] = float(np.mean(similarities))
    
    return retrieval_info


def main():
    print("=" * 80)
    print("ğŸ”¬ Chunking å‚æ•°ä¼˜åŒ–å®éªŒ")
    print("=" * 80)
    print(f"ğŸ“Š æµ‹è¯•é…ç½®æ•°é‡: {len(TEST_CONFIGS)}")
    print(f"â“ æµ‹è¯•é—®é¢˜æ•°é‡: {len(TEST_QUESTIONS)}")
    print()
    
    all_results = []
    
    for config_idx, config in enumerate(TEST_CONFIGS, 1):
        print(f"\n{'='*80}")
        print(f"ğŸ“‹ é…ç½® {config_idx}/{len(TEST_CONFIGS)}: {config['name']}")
        print(f"   chunk_size={config['chunk_size']}, chunk_overlap={config['chunk_overlap']}")
        print(f"{'='*80}")
        
        collection_name = f"test_chunk_{config['chunk_size']}_{config['chunk_overlap']}"
        
        try:
            # åˆ›å»ºç´¢å¼•
            db, embeddings, num_chunks = create_index_with_params(
                config['chunk_size'],
                config['chunk_overlap'],
                collection_name
            )
            
            config_result = {
                "config": config,
                "num_chunks": num_chunks,
                "questions": []
            }
            
            # æµ‹è¯•æ¯ä¸ªé—®é¢˜
            for question in TEST_QUESTIONS:
                print(f"\n  â“ æµ‹è¯•é—®é¢˜ ID {question['id']}...")
                retrieval_info = test_retrieval(db, embeddings, question, question['id'])
                config_result["questions"].append(retrieval_info)
                
                print(f"    æœ€é«˜ç›¸ä¼¼åº¦: {retrieval_info['max_similarity']:.4f}")
                print(f"    å¹³å‡ç›¸ä¼¼åº¦: {retrieval_info['avg_similarity']:.4f}")
                print(f"    Top-1 é¡µç : {retrieval_info['top_docs'][0]['page']}")
            
            all_results.append(config_result)
            
        except Exception as e:
            print(f"  âŒ é…ç½®æµ‹è¯•å¤±è´¥: {e}")
            continue
    
    # ä¿å­˜ç»“æœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = f"chunking_test_results_{timestamp}.json"
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2)
    
    print(f"\n\n{'='*80}")
    print(f"âœ… å®éªŒå®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    print(f"{'='*80}")
    
    # æ‰“å°å¯¹æ¯”åˆ†æ
    print("\n\nğŸ“Š é…ç½®å¯¹æ¯”åˆ†æ")
    print("=" * 80)
    print(f"{'é…ç½®':<20} {'æ–‡æ¡£å—æ•°':<10} {'ID 5 ç›¸ä¼¼åº¦':<15} {'ID 7 ç›¸ä¼¼åº¦':<15}")
    print("-" * 80)
    
    for result in all_results:
        config_name = result['config']['name']
        num_chunks = result['num_chunks']
        
        q5_result = next((q for q in result['questions'] if q['question_id'] == 5), None)
        q7_result = next((q for q in result['questions'] if q['question_id'] == 7), None)
        
        q5_sim = f"{q5_result['max_similarity']:.4f}" if q5_result else "N/A"
        q7_sim = f"{q7_result['max_similarity']:.4f}" if q7_result else "N/A"
        
        print(f"{config_name:<20} {num_chunks:<10} {q5_sim:<15} {q7_sim:<15}")
    
    # æ‰¾å‡ºæœ€ä½³é…ç½®
    print("\n\nğŸ† æœ€ä½³é…ç½®æ¨è")
    print("=" * 80)
    
    best_for_q5 = max(all_results, 
                      key=lambda r: next((q['max_similarity'] for q in r['questions'] if q['question_id'] == 5), 0))
    best_for_q7 = max(all_results,
                      key=lambda r: next((q['max_similarity'] for q in r['questions'] if q['question_id'] == 7), 0))
    
    print(f"é—®é¢˜ 5 æœ€ä½³é…ç½®: {best_for_q5['config']['name']}")
    q5_best = next(q for q in best_for_q5['questions'] if q['question_id'] == 5)
    print(f"  ç›¸ä¼¼åº¦: {q5_best['max_similarity']:.4f}")
    print(f"  æ–‡æ¡£å—æ•°: {best_for_q5['num_chunks']}")
    
    print(f"\né—®é¢˜ 7 æœ€ä½³é…ç½®: {best_for_q7['config']['name']}")
    q7_best = next(q for q in best_for_q7['questions'] if q['question_id'] == 7)
    print(f"  ç›¸ä¼¼åº¦: {q7_best['max_similarity']:.4f}")
    print(f"  æ–‡æ¡£å—æ•°: {best_for_q7['num_chunks']}")


if __name__ == "__main__":
    main()
